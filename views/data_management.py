import streamlit as st
import pandas as pd
from datetime import datetime
from data.loader import DataLoader
from config.settings import settings
import time

# Placeholder for Google OAuth Verification
# In production, use streamlit-oauth or custom httpx flow
def verify_google_oauth():
    """Mocks a Google OAuth verification flow."""
    if "is_authenticated" not in st.session_state:
        st.session_state.is_authenticated = False
        st.session_state.user_email = None

    if not st.session_state.is_authenticated:
        st.warning("üîê Administrator Access Required")
        st.info("This section is gated by Google OAuth for security compliance.")
        
        # Simulated Google Login Button
        if st.button("Sign in with Google"):
            # This would redirect to Google and callback
            with st.spinner("Authenticating with Google Accounts..."):
                time.sleep(1.5)
                # Successful Auth Mock
                st.session_state.is_authenticated = True
                st.session_state.user_email = "admin@hospital-group.com"
                st.rerun()
        return False
    return True

def render():
    """Renders the data management page with OAuth gating."""
    
    st.header("‚öôÔ∏è Data Management")
    st.markdown("---")

    # 1. Gate the entire view with OAuth
    if not verify_google_oauth():
        st.stop()

    # 2. Authenticated Admin View
    st.success(f"Logged in as: {st.session_state.user_email}")
    if st.button("Logout"):
        st.session_state.is_authenticated = False
        st.session_state.user_email = None
        st.rerun()

    st.markdown("---")

    # 3. Initialize Loader
    loader = DataLoader()
    
    # 4. Data Upload (Gated by Auth)
    st.subheader("üì§ Upload Financial Data")
    uploaded_file = st.file_uploader("Upload CSV or XLSX Hospital Data", type=["csv", "xlsx"])
    if uploaded_file:
        st.info(f"Processing {uploaded_file.name}...")
        # Processing logic would go here
        st.success("File processed and validated against schema.py")

    st.markdown("---")

    # 5. Data Source Status
    st.subheader("üì° Connection Status")
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"**Primary Source:** Local S3 / CSV")
        st.write(f"**Last Sync:** `{datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}`")
    with col2:
        if st.button("üîÑ Trigger Sync Now"):
            with st.spinner("Synchronizing with data stack..."):
                df, report = loader.load_from_csv()
                if report.status == "success":
                    st.success(f"Successfully loaded {report.total_rows} rows!")
                    st.session_state['data_quality_report'] = report
                else:
                    st.error(f"Sync failed: {report.validation_errors}")

    st.markdown("---")

    # 6. Data Quality Report
    st.subheader("üìã Quality Integrity Report")
    
    if 'data_quality_report' in st.session_state:
        report = st.session_state['data_quality_report']
        
        col1, col2, col3 = st.columns(3)
        with col1:
             st.metric("Total Records", f"{report.total_rows:,}")
        with col2:
             st.metric("Valid Integrity", f"{report.valid_rows:,}")
        with col3:
             st.metric("Data Gaps", f"{report.invalid_rows:,}")
             
        if report.validation_errors:
            with st.expander("View Anonymized Validation Findings"):
                for error in report.validation_errors:
                    st.write(f"- ‚ö†Ô∏è {error}")
    else:
        st.info("No active quality reports found. Trigger a sync to analyze dataset.")

    st.markdown("---")
    
    # 7. Preview Data
    st.subheader("üìë Processed Data Sample")
    df = loader.refresh_data()
    if not df.empty:
        st.dataframe(df.head(20), use_container_width=True)
